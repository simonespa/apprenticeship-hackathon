{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4856f239-2302-4dec-a5b9-2413762b3ae6",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "The L7 AI Hackathon will give you an opportunity to work in a team of four/five to test your new data science and ML skills. You will be required to deliver a 15mins presentation (10mins presentation and 5mins for Q&A) about your learning journey in the programme & the outcome of your work.\n",
    "\n",
    "You are going to be provided with a sample of possible questions to investigate and tasks to undertake. However, you are free to design your own tasks as a team and augment the initial datasets for your own analysis.\n",
    "\n",
    "In this hackathon, we want you to build machine learning models to predict COVID-19 infections from symptoms. It has several applications – for example, triaging patients to be attended by a doctor or nurse, recommending self-isolation through contact tracing apps, etc.\n",
    "\n",
    "Zoabi et al. ([link here](https://www.nature.com/articles/s41746-020-00372-6)) [1] builds a decision tree classifier using the publicly available data reported by the Israeli Ministry of Health. The paper itself discusses the various challenges encountered in deploying such a model. We encourage you to read the paper and learn the challenges and ways to overcome them. We will be using the dataset in this paper (Github link in the references).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Zoabi, Y., Deri-Rozov, S. & Shomron, N. Machine learning-based prediction of COVID-19 diagnosis based on symptoms. npj Digit. Med. 4, 3 (2021).\n",
    "\n",
    "[2] [Github link](https://github.com/nshomron/covidpred/tree/master) referenced in the paper. The main page of the Github repository mentions that the data file, `corona_tested_individuals_ver_006.english.csv.zip` was the version they used for the analysis. It is also the version we will use for this hackathon.\n",
    "\n",
    "## Presentation\n",
    "\n",
    "On Day 2, between 3-5PM, your team will deliver a presentation of your findings and experience for 10 minutes followed by 5 minutes of Q&A from your peers.\n",
    "\n",
    "The presentation should cover (non-exhaustively) the following components:\n",
    "\n",
    "- Briefly define the problem\n",
    "- Briefly describe the dataset\n",
    "- What did you learn about various models/techniques/etc.?\n",
    "- What’s the AUC score of your final model?\n",
    "\n",
    "Below are a list of potential questions and directions that you can explore but you are free and encouraged to be creative and explore other directions to apply your ML skills\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "- Think about possible biases and limitations of this dataset. What are the sources of uncertainty?\n",
    "- What is the format of feature values?\n",
    "- What is the statistics of these feature values? How many symptoms are reported or not?\n",
    "- Which symptoms have a reporting bias, i.e., likely to be reported when the patient is COVID positive?\n",
    "- How will the symptoms with reporting bias affect the model’s performance?\n",
    "- Visualization: Draw the bar graph of features grouped by the target class?\n",
    "- What does the bar graph of the symptoms with reporting bias look like?\n",
    "- Determine if we have a class imbalance in the dataset? If so, what do you reckon will be the downstream challenges in evaluating the model? How will you overcome those challenges?\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "- How will you represent the features in numerical format that can be accessible by model?\n",
    "- Are there any redundancies in your feature representation?\n",
    "\n",
    "## Models\n",
    "\n",
    "- Check out various classifiers in sklearn or any other preferred library to build your models\n",
    "\n",
    "## Evaluate\n",
    "\n",
    "- Is accuracy the right metric to evaluate the model? Are inaccuracies correctly penalized in the accuracy metric?\n",
    "- Which dataset should you choose to evaluate the model? Validation or Test?\n",
    "- What other metric is relevant in our context?\n",
    "\n",
    "For benchmarking everyone’s results we will stick to ROC-AUC score as a metric.\n",
    "\n",
    "## Report your result\n",
    "\n",
    "- With the metric chosen, report your result on the test dataset.\n",
    "- How will you select the threshold for your model above which model score will be interpreted as a prediction of positive diagnosis.\n",
    "\n",
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02777a09-fe44-4484-9ce9-a2c7176b2373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
